{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO: You Only Look Once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titles of the articles are You only look once: Unified Real-Time Object detection whose authors are Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi. The main purpose of this article is to present a new object detection algorithm (YOLO) with a different approach and other two are YOLO9000: Better, Faster and stronger and YOLOv3: An Incremental Improvement whose authors are Joseph Redmon and Ali Farhadi. The purpose of these two articles is just the improvement which covers the loopholes of the presented algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO has a simple and straightforward process in detecting the object as mentioned in the paper as follows\n",
    "*\tRe-size the image: Dividing the image into S*S Grids.\n",
    "*\tRuns a single Convolutional neural network: The bounding boxes B are predicted at different locations in the image and predict the class of the object.\n",
    "*\tPerforms Non-Max suppression: It is a process where most of the bounding boxes are supressed which has not-maximum Intersection over Union.\n",
    "* **Intersection over union**: This convey how much of the ground truth has common with the predicted bounding box. this has a maximum limit of 1.\n",
    "                Intersection of union = Area of overlap/area of union\n",
    "The above-mentioned steps are to be considered as a broader and high-level view of YOLO algorithm while we have more depth to be explored which leads to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNDERSTANDING YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand YOLO algorithm there are some concepts to be known like Object localisation, Object classification, Bounding box specification, Non-Max suppression, Loss functions.\n",
    "\n",
    "* **object Classification**: It is a pre-trained method where a particular object is classified by passing them to CNN’s with appropriate labels.\n",
    "The pictures (Training set) are fed into the convnets and output is given by SoftMax function where it gives the output in form of a matrix.\n",
    "\n",
    "    * **Example** :  if a self-driving car is being built it has to detect various object on the road like \n",
    "     1. Pedestrians \n",
    "     2. Car \n",
    "     3. Motorcycle \n",
    "     4. Background\n",
    "     \n",
    "\n",
    "\n",
    "       If the object is car then the result matrix will be\n",
    "       \n",
    " \\begin{equation} (0,1,0,0)  =   (c1,c2,c3,c4 )  \\end{equation}\n",
    " \n",
    "\n",
    "* **Object Localization**: The term object localization refers to figuring out where in the picture the object present. These are predicted by sending the images to convnets with already pre-defined ground truth bounding box done manually. bounding box have 4 components $ b_{x},b_{y},b_{w},b_{h} $ where $ b_{x} $  and $b_{y} $ are the midpoint of the bounding box, $ b_{w} $ and $ b_{h} $ are the width and height of bounding box. \n",
    "    \n",
    "     ![image.png](Car.png)\n",
    "                 Fig-2:Bounding box ground truth of he car\n",
    "      \n",
    "    So, input will be in the form of matrix which has 4 components namely  \\begin{bmatrix}b_{x}\\\\b_{y}\\\\b_{w}\\\\b_{h} \\end{bmatrix}\n",
    "    \n",
    "\n",
    "* **Object Classification with localization**: In this process both classification and localization happens simultaneously which results in the following matrix including 4 classes (pedestrians, Car, Motorcycle, background) and $ b_{x}, b_{y}, b_{h} $ and $ b_{w} $ there will be an extra component with these called $ p_{c} $ which conveys the probability of the object present in the picture\n",
    "    The final output (target label y) will be\n",
    "   \n",
    "  \\begin{bmatrix} p_{c}\\\\b_{x}\\\\b_{y}\\\\b_{h}\\\\b_{w}\\\\c_{1}\\\\c_{2}\\\\c_{3}  \\end{bmatrix}\n",
    "\n",
    "    If $ p_{c} = 0 $ then other numbers in the output will not be cared as object is not present in the picture.\n",
    "    We don’t have $ c_{4} $ because if the remaining three classes are 0 then indirectly classified as fourth class (Background).\n",
    "    Example: as per the given example above in fig 2 the input must be following \n",
    "\n",
    "\\begin{bmatrix}1\\\\b_x\\\\b_y\\\\b_h\\\\b_w\\\\0\\\\1\\\\0\\end{bmatrix}\n",
    "\n",
    "* **Loss Functions**:  These can be defined as the function which calculates the loss (difference) between the ground truth and predicted bounding box.\n",
    "   \n",
    "   **Sum-Squared Error**: The multipart function used in yolo stated as equation 3\n",
    "    ![LossFunction](LossFunction1.png)\n",
    "    S = number of grids, B = Bounding boxes\n",
    "    \n",
    "    **Classification loss**:\n",
    "    ![image.png](LossFunction2.png)\n",
    "    $ 1_{i}^{obj} $= 1 if object appears in cell I, otherwise 0.\n",
    "    \n",
    "    $\\hat{p_{i}}(c) $= the conditional class probability of class c in cell i.\n",
    "    \n",
    "    **Localization loss**: only consider the boxes containing object \n",
    "    ![image.png](LossFunction3.png)\n",
    "    $ 1_{i}^{obj} $ = 1 if $ j^{th} $ boundary box in cell I is responsible for predicting the boundary box, otherwise 0\n",
    "   \n",
    "   $ λ_{coord} $ increase the weight for the loss in boundary box co-ordinates\n",
    "\n",
    "    $(x_{i}, y_{i},w_{i},h_{i})$ = ground truth co-ordinated of bounding box\n",
    "    \n",
    "    $(\\hat{x}_{i}, \\hat{y}_{i},\\hat{w}_{i},\\hat{h}_{i})$ = predicted bounding box co-ordinates.\n",
    "    \n",
    "    Yolo uses square root for width and heigh instead of actual width and height and $ λ_{coord} $ because it doesnt want to calculte equal errors for small and large boxes where a small error in large box doesnt make difference but it makes a huge difference for small box\n",
    "    \n",
    "    **Confidence Loss**: if the object is in the box then\n",
    "     ![image.png](LossFunction4.png)\n",
    "      \n",
    "     $ C_{i} $ = confidence score of box j in cell i\n",
    "\n",
    "     $ 1_{i}^{obj} $ = 1 if $ j^{th} $ boundary box in cell I is responsible for predicting the boundary box, otherwise 0\n",
    "            When the object is not present\n",
    "    ![image.png](LossFunction5.png)\n",
    "    $ λ_{noobj} $= wheighs down the loss when detcting background\n",
    "    \n",
    "    The reason for using sum squared error is it weighs the localization error eqaully with classification error and in many grid cells do not contain object which pushes the confidnece score to 0 which can lead to model instability, to remedy this problem increased loss of bounding box prediction and decresed loss of conifdence prediction for boxes do not contain objects with help of two parametses λ_noobj.[4]\n",
    "    \n",
    "    Mainly this loss function is used to \n",
    "\n",
    "*    **Bounding Box specification and prediction**: All the bounding boxes are predicted by performing object classification and localization on each cell, which results in total S$*$S$*$(B$*$5+C) tensor\n",
    "    \n",
    "    Example: a Picture which has been divided into 7 grids and has 2 bounding boxes for each grid with 10 classes the output predictions will be  7$*$7$*$20 tensor.\n",
    "    all the co-ordinates of the bounding box are normalised so that they lie between 0 and 1. \n",
    "    \n",
    "       **Non_Maxima Supression**: This is a process of Supressing the prediction which has minimun confidnece scores.\n",
    "\n",
    "    **Working**:\n",
    "      Each bounding box has following \n",
    "      \\begin{bmatrix}p_c\\\\b_x\\\\b_y\\\\b_h\\\\b_w\\end{bmatrix}\n",
    "      * Discard all the boxes which p_c is less than 0.6\n",
    "      * Pick the ones which have highest p_c and output as prediction\n",
    "      * Discard any other with IOU is less than 0.5\n",
    "   \n",
    "   ![image.png](Architecture.png)\n",
    "   \n",
    "        \n",
    "        The Architecture of yolo which has 24 conv layers for which first 20 layers are pretrained with ImageNet dataset followed by Average-pooling and Fully connected layer and then converted the model for detection by adding 4 conv layers and 2 FC layers with randomly initialised weights with also increased resolution to 448*448 for detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO was developed in 2015 since then innovation happened till recently with 2 more Iteration to improve the algorithm and cover the loopholes and errors that YOLO had at that time\n",
    "\n",
    "Innovation happened by introducing \n",
    "\n",
    "YOLO9000 and YOLOv3\n",
    "    \n",
    "* **YOLO9000**\n",
    "        Improvemnets made for this version as follows:\n",
    "        * Batch Normalization\n",
    "        * High Resolution Classifier\n",
    "        * Convolutions with Anchor Box\n",
    "        * Dimesion Clusters\n",
    "        * Direct Location Prediction\n",
    "        * Fine Grained Features\n",
    "        * Multi-Scaling Training\n",
    "    **Batch Normalisation**:Batch normalization happens after convolutional layer before activation function. The main purpose to use this to keep the data clean every layer so that data stop detriorating each layer. This ideally puts the input layer between 0 and 1(Not too much variance) which has improved mean average precision by 2%. it also helps regularize model.[6]\n",
    "\n",
    "    **High-Resolution Classifier and fine grained feature**: YOLOv2 first fine tune classification network with 448*448 for 10 epochs on imagenet which gives network time to adjust its filters to work better on higher resolution we then fine tune the resulting network on detction.This helps to increase mean average precsion by 4%.\n",
    "\tIdea of fine grained feature is to reduce the spatial dimensions and increase the depth of the image which gives access to wide range of features to capture details without increasing the spatial size.\n",
    "\n",
    "    **Convolutional with Anchor boxes**: The advantage of predicting the bounding boxes using anchor boxes is instead of predicting the co-ordinates they can predict the offsets with respect to anchor boxes with which diversity among predictions can be maintained by focussing on each shape. Fully connected layers are removes and anchor boxes are used and pooling layer is elimintaed to have high resolution ouptut. \n",
    "    \n",
    "    Operation on 416*416 happens because of the odd number of saptial dimension in feature map so that there would be a single cell in the middle which has high chances for the presnece of large objects in middle of the image.\n",
    "    \n",
    "    Now class prediciton and abject detection moved form grid level to anchor box level where measuring IOU with the ground truth and proposed bounding box still remains.\n",
    "\n",
    "     **Dimesion Cluster**:Dimension clusters solves first issue using anchor \tboxes which is picking anchor boxes randomly by hand based on the shapes of different kind of objects, but instead of \tchoosing by hand running k-means clustering on training set bounding boxes to automatically find good anchor boxes.Using standard k-means with euclidean distance generates greater errors for larger boxes than smaller ones which can be solved by IOU relative to each other. K=5 is considered as a good tradeoff between model complexity and high recall. Ideally decided 5 bounding boxes for eahc cell would be good decision for better \tresults.\n",
    "\n",
    "    **Direct Location Prediction and multi scale training**:Direct location prediction solves second issue using anchor boxes with YOLO is model instability. The network predicts 5 bounding boxes per each grid cell which has 5 co-ordinates $ t_{x}, t_{y},t_{w},t_{h} $  and $ t_{0} $ and if the cell is offset from top-left corner by $ (c_{x}, c_{y}) $ and anchor box has width and height of $ p_{w} $ and $ p_{h} $ then bounding box prediction will be\n",
    "![image.png](DirectLocation.png)\n",
    "\n",
    "    The co-ordinates of the bounding box will be the sum of offset range of the grid cell and the co-ordinate applied with sigoid function which coresponds to [0,1].\n",
    "\n",
    "    The width and height of the bounding box will be the product of height and width of anchor box to the corresponding bounding box to the exponents.\n",
    "\n",
    "![image.png](DirectLocation2.png)\n",
    "    \n",
    "    Instead of having fixed input image size Yolov9000 runs in different images sizesand chooses random image for every few batches and downsamples it by factors of 32 which forces network to learn across different dimensions. And works as data augmnetation.\n",
    "    Yolo9000 can predict 9000 classes because of hierachial classification where yolo combines all the labels of different datasets to form WorkdTree.\n",
    "\n",
    "* **YOLOv3**\n",
    "    \n",
    "    Improvements made to the previous versions affects the following\n",
    "    \n",
    "    **Class Prediction**: Till now the class predictions meant to be mutually exclusive where the maximum probaility of a certain class will be 1 with softmax function. But in yolov3 replaces softmax with independent logistic classifiers to claculate the likeliness of the imput belongs to specific class.instead of using mean squared error for classification loss binary cros entropy is used for each label which also reduces computation.\n",
    "    Binary Cross entropy:\n",
    "    ![image.png](BinaryCE.png)\n",
    "    \n",
    "    Where y is the label and p(y) is the predicted probability of a specific class.\n",
    "   \n",
    "   If y=1 it adds the log(p(y)) to the loss, conversely it adds (1-p(y)) if y=0.[5]\n",
    "\n",
    "    **Bounding box prediction**:this version of yolo predicts each bounding objectness score using logistic regression, the value will be 1 if this bounding boxprior overlaps with ground truth by any other prior, if the prior overlaps a bit with the ground truth surpassing some treshold then it will be ignored.if no bounding box priors assigned to ground truth then it incure no loss or class prediction.\n",
    "    \n",
    "    **Prediction across sales using Feature pyramid Networks**: \n",
    "           * This version of yolo makes predictions at 3 different scales In the last feature map and goes back 2 layers back and upsamples it by 2 retrieves the feature map and merge it with upsampled feature map using element-wise addition and apply conv nets to make second set prediction\n",
    "           \n",
    "           * It repeates the above step until the result feature map has high-level semantic information. Yolov3 applies k-means cluster ot determine and pre-selects the cluster.\n",
    "\n",
    "    **Darkenet-53 replaces darknet-19 for the sake of speed delivering same accuracy, this pseed is achieved because darknet-52 has less billion floating point operations which save stime and computation and composed of 3*3 and 1*1 filter decreases size but increases depth of the image.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These papers are embeded with a high level of technicality. although these articles have around 5-7 pages the rectification of technical errors and improvements conveys the attention to technical aspects of the algorithm. The positive side of these papers is the comparision of this algorithm with different level state of the art algorithms with different datasets and its own previous versions showing the technical imporvement in terms of speed,accuracy,computation and complexity. \n",
    "Statistics mentioned in the papers show us show us that the by each iteration how technical quality improved with time.\n",
    "On the other side Yolo: An Incremental improvement paper stated the drwbacks on its algorithm where it struggles to detect medium or larger objects for which reasons or explanations are not disclosed, whereas in yolo:you look only once has stated its drawbacks and gave reasons of occurance.\n",
    "\n",
    "If the reasons for the yolov3 drawbacks would have been given then there would be a huge scope of research by different kinds of users which can lead to improved technicality.\n",
    "drawbacks of YOLO were considered for the seoncd and third versions where the drawbacks have become the working technical aspects until YOLOv3 but in this iterating process the strong technical aspects of YOLOv1 have become drawbacks of YOLOv3.\n",
    "\n",
    "Another Topic worth discussion is the introduction to anchor boxes, as mentioned above in innovation using anchor boxes in YOLO creates two issues which are model instability and picking priors by hand which have been dealt reasonably in YOLO9000 resulting a bit of complication, after later versions as mentioned above paragraph we found out YOLOv3 have succesfully overcome its drawbacks but created a new ones which used to be perfectly functional which brigs a question \"whether using Anchor boxes is a right choice or not?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application and X-factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined application and technique of this topic is to get the input and interpret it and produce th eoutput in the form of matrix which can be later plotted as co-ordinates. However this algorithm is used to detect the objects in the image and classify them which has been done in different and more efficent ways in those three papers.\n",
    "The main drawbacks after first version have been caught that the algorithm cannot detect small objects and struggles to generalise the objects in new or unusual aspect ratios and training of loss function which approximates detection performance leads greater effect on IOU.\n",
    "However in the later version use of anchor boxes,changes in the architecture and many other chnages have covered the errors and improved the accuracy and speed of the algortihm which led to increase in efficiency of the algortihm.\n",
    "The third iteration of the algorithm has an X-Factor of having a bit bigger network but have managed to achieve good accuracy and speed acompared to other two versions and yolov3 cannot detect large or medium size object which is quite opposite to the first version of YOLO, we can actually consider that yolov3 is a actual incremental imrovement that doesn’t much have any drwabacks or considerable errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three papers had a decent structure and reasonably good explanation of three versions of YOLO. The structre includes abstract, introduction and explanation of the algorithm followed by limitations and coclusion whereas the later two had abstract, introduction and the imporvements aspects, comparisions and conclusions. All the comparisions of accuracy with other algorithms had basis and proofs.\n",
    "However the YOLO:you only look once article is bit hard to imagine and understand as it consist with new approach to object detection and other two articles are mere improvements of the previous ones with more performance statistics. Some pre-requisites for this articles can be some understanding about object detction,localization and classification.Very high-level of technical terminology used in these three articles which can be hard and ambiguous  to understand some of the core concepts of this algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]\tJoseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, ’You Only Look Once: Unified, Real-Time Object Detection’, 2016 <https://arxiv.org/pdf/1506.02640.pdf>\n",
    "\n",
    "[2]\tJoseph Redmon, Ali Farhadi, ’ YOLO9000: Better, Faster, Stronger’ 2016 <https://arxiv.org/pdf/1612.08242.pdf>\n",
    "\n",
    "[3]\tJoseph Redmon, Ali Farhadi, ‘YOLOv3: An Incremental Improvement’ 2018 <https://arxiv.org/pdf/1804.02767.pdf>\n",
    "\n",
    "[4]\tJonathan Hui,’ Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3 , 2018         <’https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088>\n",
    "\n",
    "[5]\tDaniel Godoy, ‘Understanding binary cross-entropy / log loss: a visual explanation’, 2018    <https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a>\n",
    "\n",
    "[6]\tmachineThink, ‘Real-time object detection with YOLO’,2017\n",
    "<https://machinethink.net/blog/object-detection-with-yolo/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Link to Github:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
